{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e53f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning.utilities import rank_zero_info\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43cbec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb6d47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN = \"/deep/group/aihc-bootcamp-fall2021/lymphoma/processed/data_splits/train.hdf5\"\n",
    "PATH_TO_VAL = \"/deep/group/aihc-bootcamp-fall2021/lymphoma/processed/data_splits/val.hdf5\"\n",
    "PATH_TO_TEST = \"/deep/group/aihc-bootcamp-fall2021/lymphoma/processed/data_splits/test.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3f4389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, hdf5_path: str):\n",
    "        \n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.h5data = h5py.File(self.hdf5_path, \"r\")\n",
    "        self.cores = list(self.h5data.keys())\n",
    "        \n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cores)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        patient_id = self.cores[idx]\n",
    "        \n",
    "        patches = self.h5data[patient_id][()]\n",
    "        label = self.h5data[patient_id].attrs[\"label\"]\n",
    "        \n",
    "        if len(patches) < 8:\n",
    "            return [self.transform(im) for im in patches], torch.tensor(label)\n",
    "        \n",
    "        return random.sample([self.transform(im) for im in patches], 8), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67774f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_dataset = HDF5Dataset(PATH_TO_TRAIN)\n",
    "val_dataset = HDF5Dataset(PATH_TO_VAL)\n",
    "test_dataset = HDF5Dataset(PATH_TO_TEST)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9021563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Fine-tune Model ##########\n",
    "# Model adapted from net.py as the template model to load pretrained weights\n",
    "class TripletNet_Finetune(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TripletNet_Finetune, self).__init__()\n",
    "\n",
    "        # set the model\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        model.fc = torch.nn.Sequential()\n",
    "        self.model = model\n",
    "        self.fc = nn.Sequential(nn.Linear(512*2, 512),\n",
    "                                 nn.ReLU(True), nn.Linear(512, 256))\n",
    "\n",
    "    def forward(self, i):\n",
    "\n",
    "        E1 = self.model(i)\n",
    "        E2 = self.model(i)\n",
    "        E3 = self.model(i)\n",
    "\n",
    "        # Pairwise concatenation of features\n",
    "        E12 = torch.cat((E1, E2), dim=1)\n",
    "        E23 = torch.cat((E2, E3), dim=1)\n",
    "        E13 = torch.cat((E1, E3), dim=1)\n",
    "\n",
    "        f12 = self.fc(E12)\n",
    "        f23 = self.fc(E23)\n",
    "        f13 = self.fc(E13)\n",
    "\n",
    "        features = torch.cat((f12, f23, f13), dim=1)\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b91d95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_PRETRAINED = '/deep/group/aihc-bootcamp-fall2021/lymphoma/models/Camelyon16_pretrained_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e498e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedBaseline(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 num_classes, \n",
    "                 batch_size: int, \n",
    "                 lr: float, \n",
    "                 num_workers: int, \n",
    "                 finetune: bool = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Load pre-trained network:\n",
    "        model = TripletNet_Finetune()\n",
    "\n",
    "        state_dict = torch.load(PATH_TO_PRETRAINED) ## TODO: change this to pytorch lightning\n",
    "\n",
    "        # create new OrderedDict that does not contain `module`\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict['model'].items():\n",
    "            name = k[7:]  # remove `module.`\n",
    "            v.requires_grad = not finetune\n",
    "            new_state_dict[name] = v\n",
    "            \n",
    "        # load pretrained weights onto TripletNet_Finetune model\n",
    "        model.load_state_dict(new_state_dict)\n",
    "\n",
    "        \n",
    "        # if we finetune - only train the classifier, as opposed to e2e - freeze the network\n",
    "\n",
    "        self.feature_extractor = model\n",
    "        \n",
    "        # set the linear classifier\n",
    "        # use the classifier setup in the paper\n",
    "        self.classifier = nn.Sequential(nn.Linear(256*3, num_classes))\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # use separate metric instance for train, val and test step\n",
    "        # to ensure a proper reduction over the epoch\n",
    "        self.train_accuracy = Accuracy()\n",
    "        self.val_accuracy = Accuracy()\n",
    "        self.test_accuracy = Accuracy()\n",
    "        \n",
    "        # ensures params passed to LightningModule will be saved to ckpt\n",
    "        # allows to access params with 'self.hparams' attribute\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward step\n",
    "        x = self.feature_extractor(x).flatten(1)   # representations\n",
    "        x = self.classifier(x)                     # classifications\n",
    "        return x\n",
    "    \n",
    "    def aggregate(self, y_hats):\n",
    "        # TODO: confirm argmax/max\n",
    "        return torch.max(y_hats, dim=0)[0].unsqueeze(0)\n",
    "        \n",
    "    def infer(self, bag, y):\n",
    "        y_hats = []\n",
    "        for x in bag:\n",
    "            y_hats.append(self(x).squeeze())\n",
    "            \n",
    "        y_hat = self.aggregate(torch.stack(y_hats, dim=0))\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # only train parameters that are not frozen\n",
    "        parameters = self.parameters()\n",
    "        trainable_parameters = list(filter(lambda p: p.requires_grad, parameters))\n",
    "        \n",
    "        optimizer = torch.optim.Adam(trainable_parameters, lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        bag, y = batch\n",
    "        y_hat = self.infer(bag, y)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.train_accuracy(y_hat, y)\n",
    "    \n",
    "        # log train metrics\n",
    "        self.log(\"train/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"loss\": loss, \"preds\": y_hat.detach(), \"targets\": y.detach()}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        bag, y = batch\n",
    "        y_hat = self.infer(bag, y)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.val_accuracy(y_hat, y)\n",
    "    \n",
    "        # log validation metrics\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"loss\": loss, \"preds\": y_hat.detach(), \"targets\": y.detach()}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        bag, y = batch\n",
    "        y_hat = self.infer(bag, y)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.test_accuracy(y_hat, y)\n",
    "        \n",
    "        # log test metrics\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {\"loss\": loss, \"preds\": y_hat.detach(), \"targets\": y.detach()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afd95651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SupervisedBaseline(num_classes=9, batch_size=1, lr=1e-4, num_workers=1, finetune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8aa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name              | Type                | Params\n",
      "----------------------------------------------------------\n",
      "0 | feature_extractor | TripletNet_Finetune | 11.8 M\n",
      "1 | classifier        | Sequential          | 6.9 K \n",
      "2 | criterion         | CrossEntropyLoss    | 0     \n",
      "3 | train_accuracy    | Accuracy            | 0     \n",
      "4 | val_accuracy      | Accuracy            | 0     \n",
      "5 | test_accuracy     | Accuracy            | 0     \n",
      "----------------------------------------------------------\n",
      "11.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.8 M    Total params\n",
      "47.358    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cf3b811de6403e9c5f3e5b3f3dbcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "trainer = pl.Trainer(\n",
    "    gpus=-1, num_nodes=1, num_processes=8,\n",
    "    precision=16, limit_train_batches=0.5, accelerator=\"dp\",\n",
    "    max_epochs=10\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8015afd-38f0-48ea-9f70-a2950bfaded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext %tensorard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3681702-cdf4-46d3-9f56-5f706d14a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"../../models/test.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
